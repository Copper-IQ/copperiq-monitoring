# Grafana Unified Alerting Rules: prometheus-grafana-health
# Self-monitoring alerts for Prometheus, Grafana, and AlertManager
apiVersion: 1
groups:
  - orgId: 1
    name: prometheus-grafana-health
    folder: infrastructure
    interval: 30s
    rules:
      # Prometheus Disk Space Alerts
      - uid: prometheusdiskspacewarning
        title: PrometheusDiskSpaceWarning
        condition: C
        for: 5m
        noDataState: OK
        execErrState: Alerting
        annotations:
          summary: Prometheus disk usage high ({{ if $values.B }}{{ humanizePercentage $values.B.Value }}{{ end }})
          description: |
            Prometheus storage volume is {{ if $values.B }}{{ humanizePercentage $values.B.Value }}{{ end }} full.
            Current usage may impact data retention and new metrics ingestion.
            
            **Actions:**
            1. Check disk usage: `kubectl exec -n observability prometheus-prometheus-kube-prometheus-prometheus-0 -c prometheus -- df -h /prometheus`
            2. Consider expanding PVC or reducing retention period
            3. Review metric cardinality for high-volume targets
        labels:
          severity: warning
          component: prometheus
          category: observability
        data:
          - refId: A
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: prometheus
            model:
              expr: |
                kubelet_volume_stats_used_bytes{namespace="observability",persistentvolumeclaim=~"prometheus-.*"} 
                / kubelet_volume_stats_capacity_bytes{namespace="observability",persistentvolumeclaim=~"prometheus-.*"}
              refId: A
              datasource:
                type: prometheus
                uid: prometheus
              intervalMs: 1000
              maxDataPoints: 43200
          - refId: B
            relativeTimeRange:
              from: 0
              to: 0
            datasourceUid: __expr__
            model:
              type: reduce
              expression: A
              reducer: last
              refId: B
              datasource:
                type: __expr__
                uid: __expr__
          - refId: C
            relativeTimeRange:
              from: 0
              to: 0
            datasourceUid: __expr__
            model:
              type: math
              expression: $B > 0.75
              refId: C
              datasource:
                type: __expr__
                uid: __expr__

      - uid: prometheusdiskspacecritical
        title: PrometheusDiskSpaceCritical
        condition: C
        for: 5m
        noDataState: OK
        execErrState: Alerting
        annotations:
          summary: Prometheus disk critically full ({{ if $values.B }}{{ humanizePercentage $values.B.Value }}{{ end }})
          description: |
            ðŸš¨ CRITICAL: Prometheus storage is {{ if $values.B }}{{ humanizePercentage $values.B.Value }}{{ end }} full.
            Metrics collection will fail when disk is 100% full.
            
            **Immediate Actions:**
            1. Expand PVC: `kubectl patch pvc prometheus-prometheus-kube-prometheus-prometheus-db-prometheus-prometheus-kube-prometheus-prometheus-0 -n observability --type merge -p '{"spec":{"resources":{"requests":{"storage":"100Gi"}}}}'`
            2. Or reduce retention: Update ObservabilityStack.cs retention from 90d to 30d
            3. Monitor: `kubectl logs -n observability prometheus-prometheus-kube-prometheus-prometheus-0 -c prometheus --tail=50`
        labels:
          severity: critical
          component: prometheus
          category: observability
        data:
          - refId: A
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: prometheus
            model:
              expr: |
                kubelet_volume_stats_used_bytes{namespace="observability",persistentvolumeclaim=~"prometheus-.*"} 
                / kubelet_volume_stats_capacity_bytes{namespace="observability",persistentvolumeclaim=~"prometheus-.*"}
              refId: A
              datasource:
                type: prometheus
                uid: prometheus
              intervalMs: 1000
              maxDataPoints: 43200
          - refId: B
            relativeTimeRange:
              from: 0
              to: 0
            datasourceUid: __expr__
            model:
              type: reduce
              expression: A
              reducer: last
              refId: B
              datasource:
                type: __expr__
                uid: __expr__
          - refId: C
            relativeTimeRange:
              from: 0
              to: 0
            datasourceUid: __expr__
            model:
              type: math
              expression: $B > 0.85
              refId: C
              datasource:
                type: __expr__
                uid: __expr__

      - uid: prometheuswalcorruption
        title: PrometheusWALCorruption
        condition: C
        for: 1m
        noDataState: OK
        execErrState: Alerting
        annotations:
          summary: Prometheus WAL corruption detected
          description: |
            Prometheus Write-Ahead-Log (WAL) has corruptions.
            This typically indicates disk issues or full disk.
            
            **Actions:**
            1. Check disk health and space immediately
            2. Review pod logs for "no space left on device" errors
            3. Consider restarting Prometheus if corruption persists
        labels:
          severity: critical
          component: prometheus
          category: observability
        data:
          - refId: A
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: prometheus
            model:
              expr: increase(prometheus_tsdb_wal_corruptions_total[5m])
              refId: A
              datasource:
                type: prometheus
                uid: prometheus
              intervalMs: 1000
              maxDataPoints: 43200
          - refId: B
            relativeTimeRange:
              from: 0
              to: 0
            datasourceUid: __expr__
            model:
              type: reduce
              expression: A
              reducer: last
              refId: B
              datasource:
                type: __expr__
                uid: __expr__
          - refId: C
            relativeTimeRange:
              from: 0
              to: 0
            datasourceUid: __expr__
            model:
              type: math
              expression: $B > 0
              refId: C
              datasource:
                type: __expr__
                uid: __expr__

      # Grafana Health Alerts
      - uid: grafanadiskspacewarning
        title: GrafanaDiskSpaceWarning
        condition: C
        for: 5m
        noDataState: OK
        execErrState: Alerting
        annotations:
          summary: Grafana disk usage high ({{ if $values.B }}{{ humanizePercentage $values.B.Value }}{{ end }})
          description: |
            Grafana storage is {{ if $values.B }}{{ humanizePercentage $values.B.Value }}{{ end }} full.
            
            **Actions:**
            1. Review dashboard storage and snapshots
            2. Consider expanding Grafana PVC from 1Gi to 5Gi
        labels:
          severity: warning
          component: grafana
          category: observability
        data:
          - refId: A
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: prometheus
            model:
              expr: |
                kubelet_volume_stats_used_bytes{namespace="observability",persistentvolumeclaim=~"storage-prometheus-grafana-.*"} 
                / kubelet_volume_stats_capacity_bytes{namespace="observability",persistentvolumeclaim=~"storage-prometheus-grafana-.*"}
              refId: A
              datasource:
                type: prometheus
                uid: prometheus
              intervalMs: 1000
              maxDataPoints: 43200
          - refId: B
            relativeTimeRange:
              from: 0
              to: 0
            datasourceUid: __expr__
            model:
              type: reduce
              expression: A
              reducer: last
              refId: B
              datasource:
                type: __expr__
                uid: __expr__
          - refId: C
            relativeTimeRange:
              from: 0
              to: 0
            datasourceUid: __expr__
            model:
              type: math
              expression: $B > 0.75
              refId: C
              datasource:
                type: __expr__
                uid: __expr__

      - uid: grafanadiskspacecritical
        title: GrafanaDiskSpaceCritical
        condition: C
        for: 5m
        noDataState: OK
        execErrState: Alerting
        annotations:
          summary: Grafana disk critically full ({{ if $values.B }}{{ humanizePercentage $values.B.Value }}{{ end }})
          description: |
            ðŸš¨ CRITICAL: Grafana storage is {{ if $values.B }}{{ humanizePercentage $values.B.Value }}{{ end }} full.
            
            **Immediate Action:**
            `kubectl patch pvc storage-prometheus-grafana-0 -n observability --type merge -p '{"spec":{"resources":{"requests":{"storage":"5Gi"}}}}'`
        labels:
          severity: critical
          component: grafana
          category: observability
        data:
          - refId: A
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: prometheus
            model:
              expr: |
                kubelet_volume_stats_used_bytes{namespace="observability",persistentvolumeclaim=~"storage-prometheus-grafana-.*"} 
                / kubelet_volume_stats_capacity_bytes{namespace="observability",persistentvolumeclaim=~"storage-prometheus-grafana-.*"}
              refId: A
              datasource:
                type: prometheus
                uid: prometheus
              intervalMs: 1000
              maxDataPoints: 43200
          - refId: B
            relativeTimeRange:
              from: 0
              to: 0
            datasourceUid: __expr__
            model:
              type: reduce
              expression: A
              reducer: last
              refId: B
              datasource:
                type: __expr__
                uid: __expr__
          - refId: C
            relativeTimeRange:
              from: 0
              to: 0
            datasourceUid: __expr__
            model:
              type: math
              expression: $B > 0.90
              refId: C
              datasource:
                type: __expr__
                uid: __expr__

      - uid: grafanapoddown
        title: GrafanaPodDown
        condition: C
        for: 5m
        noDataState: Alerting
        execErrState: Alerting
        annotations:
          summary: Grafana pod is not ready
          description: |
            Grafana StatefulSet has no ready replicas. Dashboards are unavailable.
            
            **Check:** `kubectl get pods -n observability -l app.kubernetes.io/name=grafana`
        labels:
          severity: critical
          component: grafana
          category: observability
        data:
          - refId: A
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: prometheus
            model:
              expr: kube_statefulset_status_replicas_ready{namespace="observability",statefulset="prometheus-grafana"}
              refId: A
              datasource:
                type: prometheus
                uid: prometheus
              intervalMs: 1000
              maxDataPoints: 43200
          - refId: B
            relativeTimeRange:
              from: 0
              to: 0
            datasourceUid: __expr__
            model:
              type: reduce
              expression: A
              reducer: last
              refId: B
              datasource:
                type: __expr__
                uid: __expr__
          - refId: C
            relativeTimeRange:
              from: 0
              to: 0
            datasourceUid: __expr__
            model:
              type: math
              expression: $B < 1
              refId: C
              datasource:
                type: __expr__
                uid: __expr__

      # AlertManager Health
      - uid: alertmanagerpoddown
        title: AlertManagerPodDown
        condition: C
        for: 5m
        noDataState: Alerting
        execErrState: Alerting
        annotations:
          summary: AlertManager pod is not ready
          description: |
            AlertManager has no ready replicas. Alerts will not be delivered.
            
            **Check:** `kubectl get pods -n observability -l app.kubernetes.io/name=alertmanager`
        labels:
          severity: critical
          component: alertmanager
          category: observability
        data:
          - refId: A
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: prometheus
            model:
              expr: kube_statefulset_status_replicas_ready{namespace="observability",statefulset="alertmanager-prometheus-kube-prometheus-alertmanager"}
              refId: A
              datasource:
                type: prometheus
                uid: prometheus
              intervalMs: 1000
              maxDataPoints: 43200
          - refId: B
            relativeTimeRange:
              from: 0
              to: 0
            datasourceUid: __expr__
            model:
              type: reduce
              expression: A
              reducer: last
              refId: B
              datasource:
                type: __expr__
                uid: __expr__
          - refId: C
            relativeTimeRange:
              from: 0
              to: 0
            datasourceUid: __expr__
            model:
              type: math
              expression: $B < 1
              refId: C
              datasource:
                type: __expr__
                uid: __expr__
          expr: |
            (kubelet_volume_stats_used_bytes{namespace="observability",persistentvolumeclaim=~"prometheus-.*"} 
            / kubelet_volume_stats_capacity_bytes{namespace="observability",persistentvolumeclaim=~"prometheus-.*"}) > 0.85
          for: 5m
          labels:
            severity: critical
            component: prometheus
            namespace: observability
          annotations:
            summary: "Prometheus disk critically full ({{ $value | humanizePercentage }})"
            description: |
              ðŸš¨ CRITICAL: Prometheus storage is {{ $value | humanizePercentage }} full on PVC {{ $labels.persistentvolumeclaim }}.
              Metrics collection will fail when disk is 100% full.
              
              **Immediate Actions:**
              1. Expand PVC: `kubectl patch pvc {{ $labels.persistentvolumeclaim }} -n observability --type merge -p '{"spec":{"resources":{"requests":{"storage":"100Gi"}}}}'`
              2. Or reduce retention: Update ObservabilityStack.cs retention from 90d to 30d
              3. Monitor: `kubectl logs -n observability prometheus-prometheus-kube-prometheus-prometheus-0 -c prometheus --tail=50`

        - alert: PrometheusWALCorruption
          expr: |
            increase(prometheus_tsdb_wal_corruptions_total[5m]) > 0
          for: 1m
          labels:
            severity: critical
            component: prometheus
            namespace: observability
          annotations:
            summary: "Prometheus WAL corruption detected"
            description: |
              Prometheus Write-Ahead-Log (WAL) has {{ $value }} corruption(s) in the last 5 minutes.
              This typically indicates disk issues or full disk.
              
              **Actions:**
              1. Check disk health and space immediately
              2. Review pod logs for "no space left on device" errors
              3. Consider restarting Prometheus if corruption persists

        - alert: PrometheusScrapeFailures
          expr: |
            rate(prometheus_target_scrapes_exceeded_sample_limit_total[5m]) > 0
            or rate(prometheus_target_scrapes_sample_out_of_order_total[5m]) > 0
            or rate(prometheus_target_scrapes_sample_out_of_bounds_total[5m]) > 0
          for: 5m
          labels:
            severity: warning
            component: prometheus
            namespace: observability
          annotations:
            summary: "Prometheus scrape failures detected"
            description: |
              Prometheus is experiencing scrape failures: {{ $labels.job }}
              This may indicate configuration issues or target problems.
              
              **Check:** Target status at /targets endpoint

        # Grafana Health Alerts
        - alert: GrafanaDiskSpaceWarning
          expr: |
            (kubelet_volume_stats_used_bytes{namespace="observability",persistentvolumeclaim=~"storage-prometheus-grafana-.*"} 
            / kubelet_volume_stats_capacity_bytes{namespace="observability",persistentvolumeclaim=~"storage-prometheus-grafana-.*"}) > 0.75
          for: 5m
          labels:
            severity: warning
            component: grafana
            namespace: observability
          annotations:
            summary: "Grafana disk usage high ({{ $value | humanizePercentage }})"
            description: |
              Grafana storage is {{ $value | humanizePercentage }} full on PVC {{ $labels.persistentvolumeclaim }}.
              
              **Actions:**
              1. Review dashboard storage and snapshots
              2. Consider expanding Grafana PVC from 1Gi to 5Gi

        - alert: GrafanaDiskSpaceCritical
          expr: |
            (kubelet_volume_stats_used_bytes{namespace="observability",persistentvolumeclaim=~"storage-prometheus-grafana-.*"} 
            / kubelet_volume_stats_capacity_bytes{namespace="observability",persistentvolumeclaim=~"storage-prometheus-grafana-.*"}) > 0.90
          for: 5m
          labels:
            severity: critical
            component: grafana
            namespace: observability
          annotations:
            summary: "Grafana disk critically full ({{ $value | humanizePercentage }})"
            description: |
              ðŸš¨ CRITICAL: Grafana storage is {{ $value | humanizePercentage }} full.
              
              **Immediate Action:**
              `kubectl patch pvc storage-prometheus-grafana-0 -n observability --type merge -p '{"spec":{"resources":{"requests":{"storage":"5Gi"}}}}'`

        - alert: GrafanaPodDown
          expr: |
            kube_statefulset_status_replicas_ready{namespace="observability",statefulset="prometheus-grafana"} < 1
          for: 5m
          labels:
            severity: critical
            component: grafana
            namespace: observability
          annotations:
            summary: "Grafana pod is not ready"
            description: |
              Grafana StatefulSet has no ready replicas. Dashboards are unavailable.
              
              **Check:** `kubectl get pods -n observability -l app.kubernetes.io/name=grafana`

        # AlertManager Health
        - alert: AlertManagerPodDown
          expr: |
            kube_statefulset_status_replicas_ready{namespace="observability",statefulset="alertmanager-prometheus-kube-prometheus-alertmanager"} < 1
          for: 5m
          labels:
            severity: critical
            component: alertmanager
            namespace: observability
          annotations:
            summary: "AlertManager pod is not ready"
            description: |
              AlertManager has no ready replicas. Alerts will not be delivered.
              
              **Check:** `kubectl get pods -n observability -l app.kubernetes.io/name=alertmanager`

        - alert: AlertManagerConfigurationReloadFailure
          expr: |
            alertmanager_config_last_reload_successful{namespace="observability"} == 0
          for: 5m
          labels:
            severity: warning
            component: alertmanager
            namespace: observability
          annotations:
            summary: "AlertManager configuration reload failed"
            description: |
              AlertManager failed to reload its configuration. Check config syntax.
              
              **Check:** `kubectl logs -n observability alertmanager-prometheus-kube-prometheus-alertmanager-0`

    - name: prometheus.performance
      interval: 1m
      rules:
        - alert: PrometheusQueryLatencyHigh
          expr: |
            histogram_quantile(0.99, rate(prometheus_engine_query_duration_seconds_bucket[5m])) > 10
          for: 5m
          labels:
            severity: warning
            component: prometheus
            namespace: observability
          annotations:
            summary: "Prometheus query latency is high (p99: {{ $value }}s)"
            description: |
              99th percentile query duration is {{ $value | humanizeDuration }}.
              This may indicate resource constraints or complex queries.
              
              **Actions:**
              1. Check Prometheus CPU/memory usage
              2. Review slow queries in Prometheus UI
              3. Consider adding more resources or optimizing queries

        - alert: PrometheusRuleEvaluationFailures
          expr: |
            increase(prometheus_rule_evaluation_failures_total[5m]) > 0
          for: 5m
          labels:
            severity: warning
            component: prometheus
            namespace: observability
          annotations:
            summary: "Prometheus rule evaluation failures"
            description: |
              Prometheus has {{ $value }} rule evaluation failures in the last 5 minutes.
              
              **Check:** Rule syntax and resource availability
