# Grafana Unified Alerting Rules: prometheus-grafana-health
# Self-monitoring alerts for Prometheus, Grafana, and AlertManager
apiVersion: 1
groups:
  - orgId: 1
    name: prometheus-grafana-health
    folder: infrastructure
    interval: 30s
    rules:
      # Prometheus Disk Space Alerts
      - uid: prometheusdiskspacewarning
        title: PrometheusDiskSpaceWarning
        condition: C
        for: 5m
        noDataState: OK
        execErrState: Alerting
        annotations:
          summary: Prometheus disk usage high ({{ if $values.B }}{{ humanizePercentage $values.B.Value }}{{ end }})
          description: |
            Prometheus storage volume is {{ if $values.B }}{{ humanizePercentage $values.B.Value }}{{ end }} full.
            Current usage may impact data retention and new metrics ingestion.
            
            **Actions:**
            1. Check disk usage: `kubectl exec -n observability prometheus-prometheus-kube-prometheus-prometheus-0 -c prometheus -- df -h /prometheus`
            2. Consider expanding PVC or reducing retention period
            3. Review metric cardinality for high-volume targets
        labels:
          severity: warning
          component: prometheus
          category: observability
        data:
          - refId: A
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: prometheus
            model:
              expr: |
                kubelet_volume_stats_used_bytes{namespace="observability",persistentvolumeclaim=~"prometheus-.*"} 
                / kubelet_volume_stats_capacity_bytes{namespace="observability",persistentvolumeclaim=~"prometheus-.*"}
              refId: A
              datasource:
                type: prometheus
                uid: prometheus
              intervalMs: 1000
              maxDataPoints: 43200
          - refId: B
            relativeTimeRange:
              from: 0
              to: 0
            datasourceUid: __expr__
            model:
              type: reduce
              expression: A
              reducer: last
              refId: B
              datasource:
                type: __expr__
                uid: __expr__
          - refId: C
            relativeTimeRange:
              from: 0
              to: 0
            datasourceUid: __expr__
            model:
              type: math
              expression: $B > 0.75
              refId: C
              datasource:
                type: __expr__
                uid: __expr__

      - uid: prometheusdiskspacecritical
        title: PrometheusDiskSpaceCritical
        condition: C
        for: 5m
        noDataState: OK
        execErrState: Alerting
        annotations:
          summary: Prometheus disk critically full ({{ if $values.B }}{{ humanizePercentage $values.B.Value }}{{ end }})
          description: |
            ðŸš¨ CRITICAL: Prometheus storage is {{ if $values.B }}{{ humanizePercentage $values.B.Value }}{{ end }} full.
            Metrics collection will fail when disk is 100% full.
            
            **Immediate Actions:**
            1. Expand PVC: `kubectl patch pvc prometheus-prometheus-kube-prometheus-prometheus-db-prometheus-prometheus-kube-prometheus-prometheus-0 -n observability --type merge -p '{"spec":{"resources":{"requests":{"storage":"100Gi"}}}}'`
            2. Or reduce retention: Update ObservabilityStack.cs retention from 90d to 30d
            3. Monitor: `kubectl logs -n observability prometheus-prometheus-kube-prometheus-prometheus-0 -c prometheus --tail=50`
        labels:
          severity: critical
          component: prometheus
          category: observability
        data:
          - refId: A
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: prometheus
            model:
              expr: |
                kubelet_volume_stats_used_bytes{namespace="observability",persistentvolumeclaim=~"prometheus-.*"} 
                / kubelet_volume_stats_capacity_bytes{namespace="observability",persistentvolumeclaim=~"prometheus-.*"}
              refId: A
              datasource:
                type: prometheus
                uid: prometheus
              intervalMs: 1000
              maxDataPoints: 43200
          - refId: B
            relativeTimeRange:
              from: 0
              to: 0
            datasourceUid: __expr__
            model:
              type: reduce
              expression: A
              reducer: last
              refId: B
              datasource:
                type: __expr__
                uid: __expr__
          - refId: C
            relativeTimeRange:
              from: 0
              to: 0
            datasourceUid: __expr__
            model:
              type: math
              expression: $B > 0.85
              refId: C
              datasource:
                type: __expr__
                uid: __expr__

      - uid: prometheuswalcorruption
        title: PrometheusWALCorruption
        condition: C
        for: 1m
        noDataState: OK
        execErrState: Alerting
        annotations:
          summary: Prometheus WAL corruption detected
          description: |
            Prometheus Write-Ahead-Log (WAL) has corruptions.
            This typically indicates disk issues or full disk.
            
            **Actions:**
            1. Check disk health and space immediately
            2. Review pod logs for "no space left on device" errors
            3. Consider restarting Prometheus if corruption persists
        labels:
          severity: critical
          component: prometheus
          category: observability
        data:
          - refId: A
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: prometheus
            model:
              expr: increase(prometheus_tsdb_wal_corruptions_total[5m])
              refId: A
              datasource:
                type: prometheus
                uid: prometheus
              intervalMs: 1000
              maxDataPoints: 43200
          - refId: B
            relativeTimeRange:
              from: 0
              to: 0
            datasourceUid: __expr__
            model:
              type: reduce
              expression: A
              reducer: last
              refId: B
              datasource:
                type: __expr__
                uid: __expr__
          - refId: C
            relativeTimeRange:
              from: 0
              to: 0
            datasourceUid: __expr__
            model:
              type: math
              expression: $B > 0
              refId: C
              datasource:
                type: __expr__
                uid: __expr__

      # Grafana Health Alerts
      - uid: grafanadiskspacewarning
        title: GrafanaDiskSpaceWarning
        condition: C
        for: 5m
        noDataState: OK
        execErrState: Alerting
        annotations:
          summary: Grafana disk usage high ({{ if $values.B }}{{ humanizePercentage $values.B.Value }}{{ end }})
          description: |
            Grafana storage is {{ if $values.B }}{{ humanizePercentage $values.B.Value }}{{ end }} full.
            
            **Actions:**
            1. Review dashboard storage and snapshots
            2. Consider expanding Grafana PVC from 1Gi to 5Gi
        labels:
          severity: warning
          component: grafana
          category: observability
        data:
          - refId: A
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: prometheus
            model:
              expr: |
                kubelet_volume_stats_used_bytes{namespace="observability",persistentvolumeclaim=~"storage-prometheus-grafana-.*"} 
                / kubelet_volume_stats_capacity_bytes{namespace="observability",persistentvolumeclaim=~"storage-prometheus-grafana-.*"}
              refId: A
              datasource:
                type: prometheus
                uid: prometheus
              intervalMs: 1000
              maxDataPoints: 43200
          - refId: B
            relativeTimeRange:
              from: 0
              to: 0
            datasourceUid: __expr__
            model:
              type: reduce
              expression: A
              reducer: last
              refId: B
              datasource:
                type: __expr__
                uid: __expr__
          - refId: C
            relativeTimeRange:
              from: 0
              to: 0
            datasourceUid: __expr__
            model:
              type: math
              expression: $B > 0.75
              refId: C
              datasource:
                type: __expr__
                uid: __expr__

      - uid: grafanadiskspacecritical
        title: GrafanaDiskSpaceCritical
        condition: C
        for: 5m
        noDataState: OK
        execErrState: Alerting
        annotations:
          summary: Grafana disk critically full ({{ if $values.B }}{{ humanizePercentage $values.B.Value }}{{ end }})
          description: |
            ðŸš¨ CRITICAL: Grafana storage is {{ if $values.B }}{{ humanizePercentage $values.B.Value }}{{ end }} full.
            
            **Immediate Action:**
            `kubectl patch pvc storage-prometheus-grafana-0 -n observability --type merge -p '{"spec":{"resources":{"requests":{"storage":"5Gi"}}}}'`
        labels:
          severity: critical
          component: grafana
          category: observability
        data:
          - refId: A
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: prometheus
            model:
              expr: |
                kubelet_volume_stats_used_bytes{namespace="observability",persistentvolumeclaim=~"storage-prometheus-grafana-.*"} 
                / kubelet_volume_stats_capacity_bytes{namespace="observability",persistentvolumeclaim=~"storage-prometheus-grafana-.*"}
              refId: A
              datasource:
                type: prometheus
                uid: prometheus
              intervalMs: 1000
              maxDataPoints: 43200
          - refId: B
            relativeTimeRange:
              from: 0
              to: 0
            datasourceUid: __expr__
            model:
              type: reduce
              expression: A
              reducer: last
              refId: B
              datasource:
                type: __expr__
                uid: __expr__
          - refId: C
            relativeTimeRange:
              from: 0
              to: 0
            datasourceUid: __expr__
            model:
              type: math
              expression: $B > 0.90
              refId: C
              datasource:
                type: __expr__
                uid: __expr__

      - uid: grafanapoddown
        title: GrafanaPodDown
        condition: C
        for: 5m
        noDataState: Alerting
        execErrState: Alerting
        annotations:
          summary: Grafana pod is not ready
          description: |
            Grafana StatefulSet has no ready replicas. Dashboards are unavailable.
            
            **Check:** `kubectl get pods -n observability -l app.kubernetes.io/name=grafana`
        labels:
          severity: critical
          component: grafana
          category: observability
        data:
          - refId: A
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: prometheus
            model:
              expr: kube_statefulset_status_replicas_ready{namespace="observability",statefulset="prometheus-grafana"}
              refId: A
              datasource:
                type: prometheus
                uid: prometheus
              intervalMs: 1000
              maxDataPoints: 43200
          - refId: B
            relativeTimeRange:
              from: 0
              to: 0
            datasourceUid: __expr__
            model:
              type: reduce
              expression: A
              reducer: last
              refId: B
              datasource:
                type: __expr__
                uid: __expr__
          - refId: C
            relativeTimeRange:
              from: 0
              to: 0
            datasourceUid: __expr__
            model:
              type: math
              expression: $B < 1
              refId: C
              datasource:
                type: __expr__
                uid: __expr__

      # AlertManager Health
      - uid: alertmanagerpoddown
        title: AlertManagerPodDown
        condition: C
        for: 5m
        noDataState: Alerting
        execErrState: Alerting
        annotations:
          summary: AlertManager pod is not ready
          description: |
            AlertManager has no ready replicas. Alerts will not be delivered.
            
            **Check:** `kubectl get pods -n observability -l app.kubernetes.io/name=alertmanager`
        labels:
          severity: critical
          component: alertmanager
          category: observability
        data:
          - refId: A
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: prometheus
            model:
              expr: kube_statefulset_status_replicas_ready{namespace="observability",statefulset="alertmanager-prometheus-kube-prometheus-alertmanager"}
              refId: A
              datasource:
                type: prometheus
                uid: prometheus
              intervalMs: 1000
              maxDataPoints: 43200
          - refId: B
            relativeTimeRange:
              from: 0
              to: 0
            datasourceUid: __expr__
            model:
              type: reduce
              expression: A
              reducer: last
              refId: B
              datasource:
                type: __expr__
                uid: __expr__
          - refId: C
            relativeTimeRange:
              from: 0
              to: 0
            datasourceUid: __expr__
            model:
              type: math
              expression: $B < 1
              refId: C
              datasource:
                type: __expr__
                uid: __expr__
