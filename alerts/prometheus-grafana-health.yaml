apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: prometheus-grafana-health
  namespace: observability
  labels:
    prometheus: kube-prometheus
    role: alert-rules
spec:
  groups:
    - name: prometheus.health
      interval: 30s
      rules:
        # Prometheus Disk Space Alerts
        - alert: PrometheusDiskSpaceWarning
          expr: |
            (kubelet_volume_stats_used_bytes{namespace="observability",persistentvolumeclaim=~"prometheus-.*"} 
            / kubelet_volume_stats_capacity_bytes{namespace="observability",persistentvolumeclaim=~"prometheus-.*"}) > 0.75
          for: 5m
          labels:
            severity: warning
            component: prometheus
            namespace: observability
          annotations:
            summary: "Prometheus disk usage high ({{ $value | humanizePercentage }})"
            description: |
              Prometheus storage volume is {{ $value | humanizePercentage }} full on PVC {{ $labels.persistentvolumeclaim }}.
              Current usage may impact data retention and new metrics ingestion.
              
              **Actions:**
              1. Check disk usage: `kubectl exec -n observability prometheus-prometheus-kube-prometheus-prometheus-0 -c prometheus -- df -h /prometheus`
              2. Consider expanding PVC or reducing retention period
              3. Review metric cardinality for high-volume targets

        - alert: PrometheusDiskSpaceCritical
          expr: |
            (kubelet_volume_stats_used_bytes{namespace="observability",persistentvolumeclaim=~"prometheus-.*"} 
            / kubelet_volume_stats_capacity_bytes{namespace="observability",persistentvolumeclaim=~"prometheus-.*"}) > 0.85
          for: 5m
          labels:
            severity: critical
            component: prometheus
            namespace: observability
          annotations:
            summary: "Prometheus disk critically full ({{ $value | humanizePercentage }})"
            description: |
              ðŸš¨ CRITICAL: Prometheus storage is {{ $value | humanizePercentage }} full on PVC {{ $labels.persistentvolumeclaim }}.
              Metrics collection will fail when disk is 100% full.
              
              **Immediate Actions:**
              1. Expand PVC: `kubectl patch pvc {{ $labels.persistentvolumeclaim }} -n observability --type merge -p '{"spec":{"resources":{"requests":{"storage":"100Gi"}}}}'`
              2. Or reduce retention: Update ObservabilityStack.cs retention from 90d to 30d
              3. Monitor: `kubectl logs -n observability prometheus-prometheus-kube-prometheus-prometheus-0 -c prometheus --tail=50`

        - alert: PrometheusWALCorruption
          expr: |
            increase(prometheus_tsdb_wal_corruptions_total[5m]) > 0
          for: 1m
          labels:
            severity: critical
            component: prometheus
            namespace: observability
          annotations:
            summary: "Prometheus WAL corruption detected"
            description: |
              Prometheus Write-Ahead-Log (WAL) has {{ $value }} corruption(s) in the last 5 minutes.
              This typically indicates disk issues or full disk.
              
              **Actions:**
              1. Check disk health and space immediately
              2. Review pod logs for "no space left on device" errors
              3. Consider restarting Prometheus if corruption persists

        - alert: PrometheusScrapeFailures
          expr: |
            rate(prometheus_target_scrapes_exceeded_sample_limit_total[5m]) > 0
            or rate(prometheus_target_scrapes_sample_out_of_order_total[5m]) > 0
            or rate(prometheus_target_scrapes_sample_out_of_bounds_total[5m]) > 0
          for: 5m
          labels:
            severity: warning
            component: prometheus
            namespace: observability
          annotations:
            summary: "Prometheus scrape failures detected"
            description: |
              Prometheus is experiencing scrape failures: {{ $labels.job }}
              This may indicate configuration issues or target problems.
              
              **Check:** Target status at /targets endpoint

        # Grafana Health Alerts
        - alert: GrafanaDiskSpaceWarning
          expr: |
            (kubelet_volume_stats_used_bytes{namespace="observability",persistentvolumeclaim=~"storage-prometheus-grafana-.*"} 
            / kubelet_volume_stats_capacity_bytes{namespace="observability",persistentvolumeclaim=~"storage-prometheus-grafana-.*"}) > 0.75
          for: 5m
          labels:
            severity: warning
            component: grafana
            namespace: observability
          annotations:
            summary: "Grafana disk usage high ({{ $value | humanizePercentage }})"
            description: |
              Grafana storage is {{ $value | humanizePercentage }} full on PVC {{ $labels.persistentvolumeclaim }}.
              
              **Actions:**
              1. Review dashboard storage and snapshots
              2. Consider expanding Grafana PVC from 1Gi to 5Gi

        - alert: GrafanaDiskSpaceCritical
          expr: |
            (kubelet_volume_stats_used_bytes{namespace="observability",persistentvolumeclaim=~"storage-prometheus-grafana-.*"} 
            / kubelet_volume_stats_capacity_bytes{namespace="observability",persistentvolumeclaim=~"storage-prometheus-grafana-.*"}) > 0.90
          for: 5m
          labels:
            severity: critical
            component: grafana
            namespace: observability
          annotations:
            summary: "Grafana disk critically full ({{ $value | humanizePercentage }})"
            description: |
              ðŸš¨ CRITICAL: Grafana storage is {{ $value | humanizePercentage }} full.
              
              **Immediate Action:**
              `kubectl patch pvc storage-prometheus-grafana-0 -n observability --type merge -p '{"spec":{"resources":{"requests":{"storage":"5Gi"}}}}'`

        - alert: GrafanaPodDown
          expr: |
            kube_statefulset_status_replicas_ready{namespace="observability",statefulset="prometheus-grafana"} < 1
          for: 5m
          labels:
            severity: critical
            component: grafana
            namespace: observability
          annotations:
            summary: "Grafana pod is not ready"
            description: |
              Grafana StatefulSet has no ready replicas. Dashboards are unavailable.
              
              **Check:** `kubectl get pods -n observability -l app.kubernetes.io/name=grafana`

        # AlertManager Health
        - alert: AlertManagerPodDown
          expr: |
            kube_statefulset_status_replicas_ready{namespace="observability",statefulset="alertmanager-prometheus-kube-prometheus-alertmanager"} < 1
          for: 5m
          labels:
            severity: critical
            component: alertmanager
            namespace: observability
          annotations:
            summary: "AlertManager pod is not ready"
            description: |
              AlertManager has no ready replicas. Alerts will not be delivered.
              
              **Check:** `kubectl get pods -n observability -l app.kubernetes.io/name=alertmanager`

        - alert: AlertManagerConfigurationReloadFailure
          expr: |
            alertmanager_config_last_reload_successful{namespace="observability"} == 0
          for: 5m
          labels:
            severity: warning
            component: alertmanager
            namespace: observability
          annotations:
            summary: "AlertManager configuration reload failed"
            description: |
              AlertManager failed to reload its configuration. Check config syntax.
              
              **Check:** `kubectl logs -n observability alertmanager-prometheus-kube-prometheus-alertmanager-0`

    - name: prometheus.performance
      interval: 1m
      rules:
        - alert: PrometheusQueryLatencyHigh
          expr: |
            histogram_quantile(0.99, rate(prometheus_engine_query_duration_seconds_bucket[5m])) > 10
          for: 5m
          labels:
            severity: warning
            component: prometheus
            namespace: observability
          annotations:
            summary: "Prometheus query latency is high (p99: {{ $value }}s)"
            description: |
              99th percentile query duration is {{ $value | humanizeDuration }}.
              This may indicate resource constraints or complex queries.
              
              **Actions:**
              1. Check Prometheus CPU/memory usage
              2. Review slow queries in Prometheus UI
              3. Consider adding more resources or optimizing queries

        - alert: PrometheusRuleEvaluationFailures
          expr: |
            increase(prometheus_rule_evaluation_failures_total[5m]) > 0
          for: 5m
          labels:
            severity: warning
            component: prometheus
            namespace: observability
          annotations:
            summary: "Prometheus rule evaluation failures"
            description: |
              Prometheus has {{ $value }} rule evaluation failures in the last 5 minutes.
              
              **Check:** Rule syntax and resource availability
