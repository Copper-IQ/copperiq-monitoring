apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: rabbitmq-alerts
  namespace: observability
  labels:
    prometheus: kube-prometheus
    role: alert-rules
    app.kubernetes.io/name: copperiq-monitoring
    app.kubernetes.io/component: alert-rules
spec:
  groups:
    - name: rabbitmq
      interval: 30s
      rules:
        - alert: RabbitMQHighMemory
          expr: |
            rabbitmq_process_resident_memory_bytes / rabbitmq_resident_memory_limit_bytes > 0.8
          for: 5m
          labels:
            severity: warning
            component: rabbitmq
            category: infrastructure
          annotations:
            summary: "RabbitMQ {{ $labels.namespace }} memory > 80%"
            description: |
              RabbitMQ in {{ $labels.namespace }} is using {{ $value | humanizePercentage }} of memory limit.
              
              **Memory alarm threshold**: 90%
              **Impact**: At 90%, RabbitMQ will block message publishing.
              
              **Action**:
              1. Check memory details: `kubectl exec -n {{ $labels.namespace }} rabbitmq-0 -- rabbitmqctl status`
              2. Check queue lengths: `kubectl exec -n {{ $labels.namespace }} rabbitmq-0 -- rabbitmqctl list_queues name messages`
              3. Check consumer activity: `kubectl exec -n {{ $labels.namespace }} rabbitmq-0 -- rabbitmqctl list_consumers`
              4. Consider increasing memory limit if sustained

        - alert: RabbitMQMemoryAlarm
          expr: |
            rabbitmq_process_resident_memory_bytes / rabbitmq_resident_memory_limit_bytes > 0.9
          for: 2m
          labels:
            severity: critical
            component: rabbitmq
            category: infrastructure
          annotations:
            summary: "RabbitMQ {{ $labels.namespace }} memory ALARM"
            description: |
              CRITICAL: RabbitMQ in {{ $labels.namespace }} has triggered memory alarm (>90%).
              
              **Impact**: Message publishing is BLOCKED.
              
              **Immediate action**:
              1. Check alarm status: `kubectl exec -n {{ $labels.namespace }} rabbitmq-0 -- rabbitmqctl list_alarms`
              2. Purge old messages if safe: `kubectl exec -n {{ $labels.namespace }} rabbitmq-0 -- rabbitmqctl purge_queue <queue-name>`
              3. Increase memory limit: Edit StatefulSet memory resources
              4. Restart RabbitMQ: `kubectl delete pod -n {{ $labels.namespace }} rabbitmq-0` (StatefulSet will recreate)

        - alert: RabbitMQHighDiskUsage
          expr: |
            rabbitmq_disk_space_available_bytes / (rabbitmq_disk_space_available_bytes + rabbitmq_disk_space_used_bytes) < 0.2
          for: 5m
          labels:
            severity: warning
            component: rabbitmq
            category: infrastructure
          annotations:
            summary: "RabbitMQ {{ $labels.namespace }} disk > 80% full"
            description: |
              RabbitMQ in {{ $labels.namespace }} has less than 20% disk space available.
              
              **Disk alarm threshold**: Usually 50GB or 10% free
              **Impact**: At threshold, RabbitMQ will block message publishing.
              
              **Action**:
              1. Check disk alarm: `kubectl exec -n {{ $labels.namespace }} rabbitmq-0 -- rabbitmqctl list_alarms`
              2. Check PVC usage: `kubectl exec -n {{ $labels.namespace }} rabbitmq-0 -- df -h /var/lib/rabbitmq`
              3. Review message persistence settings
              4. Consider expanding PVC

        - alert: RabbitMQNodeDown
          expr: |
            up{job="rabbitmq",namespace=~"n8n.*"} == 0
          for: 2m
          labels:
            severity: critical
            component: rabbitmq
            category: infrastructure
          annotations:
            summary: "RabbitMQ {{ $labels.namespace }} is down"
            description: |
              RabbitMQ node in {{ $labels.namespace }} is not responding.
              
              **Impact**:
              - Message queue unavailable
              - n8n workers cannot consume jobs
              - Content generation stopped
              
              **Action**:
              1. Check pod status: `kubectl get pods -n {{ $labels.namespace }} rabbitmq-0`
              2. Check logs: `kubectl logs -n {{ $labels.namespace }} rabbitmq-0 --tail=200`
              3. Check events: `kubectl get events -n {{ $labels.namespace }} | grep rabbitmq`
              4. Check PVC: `kubectl get pvc -n {{ $labels.namespace }}`
              5. Restart if needed: `kubectl delete pod -n {{ $labels.namespace }} rabbitmq-0`

        - alert: RabbitMQHighFileDescriptors
          expr: |
            rabbitmq_process_open_fds / rabbitmq_process_max_fds > 0.8
          for: 5m
          labels:
            severity: warning
            component: rabbitmq
            category: infrastructure
          annotations:
            summary: "RabbitMQ {{ $labels.namespace }} file descriptors > 80%"
            description: |
              RabbitMQ in {{ $labels.namespace }} is using {{ $value | humanizePercentage }} of file descriptor limit.
              
              **Impact**: At limit, cannot accept new connections.
              
              **Action**:
              1. Check current usage: `kubectl exec -n {{ $labels.namespace }} rabbitmq-0 -- rabbitmqctl status | grep file_descriptors`
              2. Check connection count: `kubectl exec -n {{ $labels.namespace }} rabbitmq-0 -- rabbitmqctl list_connections`
              3. Look for connection leaks in n8n workers
              4. Increase file descriptor limit in StatefulSet if needed

        - alert: RabbitMQHighConnectionChurn
          expr: |
            rate(rabbitmq_connections_opened_total[5m]) > 5
          for: 10m
          labels:
            severity: warning
            component: rabbitmq
            category: infrastructure
          annotations:
            summary: "RabbitMQ {{ $labels.namespace }} high connection churn"
            description: |
              RabbitMQ in {{ $labels.namespace }} is experiencing high connection churn rate.
              
              **Connection rate**: {{ $value | humanize }} new connections/second
              
              **Impact**: Performance degradation, potential connection exhaustion.
              
              **Possible causes**:
              1. n8n workers restarting frequently
              2. Application not reusing connections
              3. Connection timeout too short
              
              **Action**:
              1. Check n8n worker restarts: `kubectl get pods -n {{ $labels.namespace }} -l app contains worker`
              2. Review n8n RabbitMQ connection configuration
              3. Check for pod crash loops
